{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "# from tensorflow.keras.preprocessing.image import img_to_array\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from imutils.video import VideoStream\n",
    "# # import numpy as np\n",
    "# import imutils\n",
    "# import time\n",
    "# # from cv2 import cv2\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load YOLO\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\",\"yolov3.cfg\") # Original yolov3\n",
    "#net = cv2.dnn.readNet(\"yolov3-tiny.weights\",\"yolov3-tiny.cfg\") #Tiny Yolo\n",
    "classes = []\n",
    "with open(\"coco.names\",\"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
     ]
    }
   ],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = net.getLayerNames()\n",
    "outputlayers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors= np.random.uniform(0,255,size=(len(classes),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detetction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maskNet = load_model(\"mask_detector.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def detect_and_predict_mask(faces, maskNet):\n",
    "# \t# grab the dimensions of the frame and then construct a blob\n",
    "# \t# from it\n",
    "# \tpreds = []\n",
    "# # \tframe = cv2.resize(face, (224, 224))\n",
    "\n",
    "# \tif len(faces) > 0:\n",
    "\n",
    "# \t\tfaces = np.array(faces, dtype=\"float32\")\n",
    "# \t\tpreds = maskNet.predict(faces, batch_size=32)\n",
    "\n",
    "# \t# return a 2-tuple of the face locations and their corresponding\n",
    "# \t# locations\n",
    "# \treturn preds\n",
    "# def mask(frame,locs,preds):\n",
    "# \t# face mask or not\n",
    "\n",
    "\n",
    "# \t# loop over the detected face locations and their corresponding\n",
    "# \t# locations\n",
    "# \tfor (box, pred) in zip(locs, preds):\n",
    "# \t\t# unpack the bounding box and predictions\n",
    "# \t\t(startX, startY, endX, endY) = box\n",
    "# \t\t(mask, withoutMask) = pred\n",
    "\n",
    "# \t\t# determine the class label and color we'll use to draw\n",
    "# \t\t# the bounding box and text\n",
    "# \t\tlabel = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
    "# \t\tcolor = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
    "\n",
    "# \t\t# include the probability in the label\n",
    "# \t\tlabel = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
    "\n",
    "# \t\t# display the label and bounding box rectangle on the output\n",
    "# \t\t# frame\n",
    "# \t\tcv2.putText(frame, label, (startX, startY - 10),\n",
    "# \t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "# \t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "# \treturn frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_Cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml')\n",
    "# face_Cascade = cv2.CascadeClassifier('haarcascade_frontalface_alt2.xml')\n",
    "# cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "# face_Cascade=face_cascade.load('haarcascade_frontalface_default.xml')\n",
    "# face_Cascade = cv2.CascadeClassifier('/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/cv2/data/haarcascade/haarcascade_frontalface_alt2.xml')\n",
    "\n",
    "def Resize(image , x, y, h, w):\n",
    "    return image[y:y+h, x:x+w]\n",
    "\n",
    "\n",
    "def face(image):\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_Cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(60, 60),\n",
    "        flags=cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "#here if len(faces)>0 then check for mask\n",
    "#     images=[]\n",
    "#     locs=[]\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(image,(x,y),(x+w,y+h),colors[0],2)\n",
    "#         cv2.putText(image,\"Face\",2)\n",
    "        cv2.putText(image,\"Face\",(x,y+15),font,1,(255,255,255),2)\n",
    "#         images.append(Resize(image,x,y,h,w))\n",
    "#         locs.append((x,y,x+w,y+h))\n",
    "#     preds=detect_and_predict_mask(images,maskNet)\n",
    "#     image=mask(image,locs,preds)\n",
    "# #         image = Resize(image, x, y, h, w )\n",
    "    \n",
    "    return image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes at 1.9296112060546875\n",
      "yes at 2.9129798412323\n",
      "yes at 3.917294502258301\n",
      "yes at 4.914928197860718\n",
      "yes at 6.000026226043701\n",
      "yes at 6.967440366744995\n",
      "yes at 7.957792520523071\n",
      "yes at 8.814502477645874\n",
      "yes at 9.752177476882935\n",
      "yes at 10.70961880683899\n",
      "yes at 11.572313785552979\n",
      "yes at 12.453956127166748\n",
      "yes at 13.290718793869019\n",
      "yes at 14.164382219314575\n",
      "yes at 15.024085283279419\n",
      "yes at 15.874554872512817\n",
      "yes at 16.743433713912964\n",
      "yes at 17.589172840118408\n",
      "yes at 18.43907642364502\n",
      "yes at 19.310744047164917\n",
      "yes at 20.154487133026123\n",
      "yes at 21.0421142578125\n",
      "yes at 21.895831823349\n",
      "yes at 22.74556040763855\n",
      "yes at 23.618227243423462\n",
      "yes at 24.484912633895874\n",
      "yes at 25.381514072418213\n",
      "yes at 26.237225770950317\n",
      "yes at 27.086955070495605\n",
      "yes at 27.985553979873657\n",
      "yes at 28.940998077392578\n",
      "yes at 29.844032287597656\n",
      "yes at 30.714704990386963\n",
      "yes at 31.603330850601196\n",
      "yes at 32.480984926223755\n",
      "yes at 33.33769345283508\n",
      "yes at 34.25244736671448\n",
      "yes at 35.08920979499817\n",
      "yes at 35.943923234939575\n",
      "yes at 36.89837312698364\n",
      "yes at 37.79098558425903\n",
      "yes at 38.75541114807129\n",
      "yes at 39.59733772277832\n",
      "yes at 40.44349956512451\n",
      "yes at 41.34823966026306\n",
      "yes at 42.252821922302246\n",
      "yes at 43.15640616416931\n",
      "yes at 43.99959206581116\n",
      "yes at 44.875003814697266\n",
      "yes at 45.73869562149048\n",
      "yes at 46.59956502914429\n",
      "yes at 47.56298899650574\n",
      "yes at 48.62245178222656\n",
      "yes at 49.86705446243286\n",
      "yes at 50.96307039260864\n",
      "yes at 52.30323910713196\n",
      "yes at 53.35203742980957\n",
      "yes at 54.41988277435303\n",
      "yes at 55.545799016952515\n",
      "yes at 56.82145023345947\n",
      "yes at 57.91105937957764\n",
      "yes at 58.86982226371765\n",
      "yes at 59.732338190078735\n",
      "yes at 60.703455686569214\n",
      "yes at 61.56723427772522\n",
      "yes at 62.406957387924194\n",
      "yes at 63.27114963531494\n",
      "yes at 64.14408278465271\n",
      "yes at 64.98972177505493\n",
      "yes at 65.84524750709534\n",
      "yes at 66.71552681922913\n",
      "yes at 67.5642101764679\n",
      "yes at 68.41905665397644\n",
      "yes at 69.28550004959106\n",
      "yes at 70.17181611061096\n",
      "yes at 71.02379894256592\n",
      "yes at 71.87263298034668\n",
      "yes at 72.74346613883972\n",
      "yes at 73.60759377479553\n",
      "yes at 74.49327063560486\n",
      "yes at 75.33306121826172\n",
      "yes at 76.1849513053894\n",
      "yes at 77.06763076782227\n",
      "yes at 77.9945912361145\n",
      "yes at 78.84601426124573\n",
      "yes at 79.72879004478455\n",
      "yes at 80.58382177352905\n",
      "yes at 81.4636025428772\n",
      "yes at 82.30511784553528\n",
      "yes at 83.15085339546204\n",
      "yes at 84.03791427612305\n",
      "yes at 84.89340567588806\n",
      "yes at 85.81306219100952\n",
      "yes at 86.65957808494568\n",
      "yes at 87.49908351898193\n",
      "yes at 88.37335634231567\n",
      "yes at 89.22179651260376\n",
      "yes at 90.10768699645996\n",
      "yes at 90.96245050430298\n",
      "yes at 91.81367897987366\n",
      "yes at 92.68203067779541\n",
      "yes at 93.53057312965393\n",
      "yes at 94.43169140815735\n",
      "yes at 95.33600115776062\n",
      "yes at 96.64524960517883\n",
      "yes at 97.69633054733276\n",
      "yes at 98.71434593200684\n",
      "yes at 100.01506042480469\n",
      "yes at 100.88527631759644\n",
      "yes at 101.79524493217468\n",
      "yes at 102.68768072128296\n",
      "yes at 103.54224896430969\n",
      "yes at 104.45917868614197\n",
      "yes at 105.43151092529297\n",
      "yes at 106.2981276512146\n",
      "yes at 107.13457465171814\n",
      "yes at 108.24898076057434\n",
      "yes at 109.1240029335022\n",
      "yes at 109.99458694458008\n",
      "yes at 110.89326548576355\n",
      "yes at 111.77979803085327\n",
      "yes at 112.6816577911377\n",
      "yes at 113.5751211643219\n",
      "yes at 114.50826740264893\n",
      "yes at 115.51838970184326\n",
      "yes at 116.47591400146484\n",
      "yes at 117.3872697353363\n",
      "yes at 118.23531126976013\n",
      "yes at 119.0906240940094\n",
      "yes at 119.93914937973022\n",
      "yes at 120.77214121818542\n",
      "yes at 121.59671068191528\n",
      "yes at 122.45066428184509\n",
      "yes at 123.28575706481934\n",
      "yes at 124.14093947410583\n",
      "yes at 124.97116732597351\n",
      "yes at 125.79096007347107\n",
      "yes at 126.6428644657135\n",
      "yes at 127.47506904602051\n",
      "yes at 128.61447501182556\n",
      "yes at 129.5697844028473\n",
      "yes at 130.43958711624146\n",
      "yes at 131.37297201156616\n",
      "yes at 132.39013481140137\n",
      "yes at 133.3124270439148\n",
      "yes at 134.39498257637024\n",
      "yes at 135.73141741752625\n",
      "yes at 136.67134284973145\n",
      "yes at 137.65267491340637\n",
      "yes at 138.49138641357422\n",
      "yes at 139.31101727485657\n",
      "yes at 140.1783709526062\n",
      "yes at 141.04253149032593\n",
      "yes at 141.9290463924408\n",
      "yes at 142.8381278514862\n",
      "yes at 143.9972529411316\n",
      "yes at 145.89211678504944\n",
      "yes at 146.7647442817688\n",
      "yes at 147.98454642295837\n",
      "yes at 149.01400089263916\n",
      "yes at 149.86180567741394\n",
      "yes at 150.7100851535797\n",
      "yes at 151.6145625114441\n",
      "yes at 152.56823706626892\n",
      "yes at 153.44793605804443\n",
      "yes at 154.38777613639832\n",
      "yes at 155.27144694328308\n",
      "yes at 156.17707061767578\n",
      "yes at 157.18713331222534\n",
      "yes at 158.139000415802\n",
      "yes at 159.22428059577942\n",
      "yes at 160.31316447257996\n",
      "yes at 161.20983743667603\n",
      "yes at 162.33189177513123\n",
      "yes at 163.24976873397827\n",
      "yes at 164.18919587135315\n",
      "yes at 165.0216121673584\n"
     ]
    }
   ],
   "source": [
    "#loading image\n",
    "cap=cv2.VideoCapture(0) #0 for 1st webcam\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "starting_time= time.time()\n",
    "frame_id = 0\n",
    "\n",
    "while True:\n",
    "    _,frame= cap.read() # \n",
    "    frame_id+=1\n",
    "    \n",
    "    height,width,channels = frame.shape\n",
    "    count_humans=0\n",
    "    count_faces=0\n",
    "    count_mask=0\n",
    "    #detecting objects\n",
    "    blob = cv2.dnn.blobFromImage(frame,0.00392,(320,320),(0,0,0),True,crop=False) #reduce 416 to 320    \n",
    "\n",
    "        \n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(outputlayers)\n",
    "    #print(outs[1])\n",
    "\n",
    "\n",
    "    #Showing info on screen/ get confidence score of algorithm in detecting an object in blob\n",
    "    class_ids=[]\n",
    "    human_ids=[]\n",
    "    face_ids=[]\n",
    "    confidences=[]\n",
    "    boxes=[]\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.3:\n",
    "                #onject detected\n",
    "                center_x= int(detection[0]*width)\n",
    "                center_y= int(detection[1]*height)\n",
    "                w = int(detection[2]*width)\n",
    "                h = int(detection[3]*height)\n",
    "\n",
    "                #cv2.circle(img,(center_x,center_y),10,(0,255,0),2)\n",
    "                #rectangle co-ordinaters\n",
    "                x=int(center_x - w/2)\n",
    "                y=int(center_y - h/2)\n",
    "                #cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "\n",
    "                boxes.append([x,y,w,h]) #put all rectangle areas\n",
    "                confidences.append(float(confidence)) #how confidence was that object detected and show that percentage\n",
    "                class_ids.append(class_id) #name of the object tha was detected\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes,confidences,0.4,0.6)\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x,y,w,h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            if(label==\"person\"):\n",
    "                count_humans=count_humans+1\n",
    "                human_ids.append(i)\n",
    "                #this is for bounding box\n",
    "                color = colors[class_ids[i]]\n",
    "                cv2.rectangle(frame,(x,y),(x+w,y+h),color,2)\n",
    "#                 frame=Resize(frame,x,y,w,h)\n",
    "                confidence= confidences[i]\n",
    "                cv2.putText(frame,label+\" \"+str(round(confidence,2)),(x,y+30),font,1,(255,255,255),2)\n",
    "            \n",
    "\n",
    "\n",
    "    elapsed_time = time.time() - starting_time\n",
    "    fps=frame_id/elapsed_time\n",
    "    cv2.putText(frame,\"FPS:\"+str(round(fps,2)),(10,50),font,2,(0,0,0),1)\n",
    "    if(count_humans>0):\n",
    "        #for each sec we are calling face if there exists any person in the camera view.\n",
    "        print(\"yes at \"+ str(elapsed_time))\n",
    "        for i in range(len(human_ids)):\n",
    "            frame = face(frame)\n",
    "    else:\n",
    "        print(\"no at \"+ str(elapsed_time))\n",
    "    \n",
    "    cv2.imshow(\"Image\",frame)\n",
    "    key = cv2.waitKey(1) #wait 1ms the loop will start again and we will process the next frame\n",
    "    \n",
    "    if key == 27: #esc key stops the process\n",
    "        break;\n",
    "    \n",
    "cap.release()    \n",
    "cv2.destroyAllWindows()    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import cv2\n",
    "\n",
    "# face_Cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml')\n",
    "\n",
    "# def Resize(image , x, y, h, w):\n",
    "#     return image[y:y+h, x:x+w]\n",
    "\n",
    "\n",
    "# def face(image):\n",
    "\n",
    "#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#     faces = face_Cascade.detectMultiScale(\n",
    "#         gray,\n",
    "#         scaleFactor=1.1,\n",
    "#         minNeighbors=5,\n",
    "#         minSize=(60, 60),\n",
    "#         flags=cv2.CASCADE_SCALE_IMAGE\n",
    "#     )\n",
    "\n",
    "#     for (x, y, w, h) in faces:\n",
    "#         image = Resize(image, x, y, h, w )\n",
    "    \n",
    "#     return image \n",
    "\n",
    "\n",
    "# def Main():\n",
    "#     Videocap = cv2.VideoCapture(0)\n",
    "\n",
    "#     while(True):\n",
    "#         ret, frame = Videocap.read()\n",
    "\n",
    "#         frame = face(frame)\n",
    "\n",
    "#         cv2.imshow('frame',frame)\n",
    "#         #press 'q' to end web cam.  \n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "#     Videocap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "# import numpy as np\n",
    "# import cv2\n",
    "\n",
    "# face_Cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml')\n",
    "\n",
    "# def Resize(image , x, y, h, w):\n",
    "#     return image[y:y+h, x:x+w]\n",
    "\n",
    "\n",
    "# def face(image):\n",
    "\n",
    "#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#     faces = face_Cascade.detectMultiScale(\n",
    "#         gray,\n",
    "#         scaleFactor=1.1,\n",
    "#         minNeighbors=5,\n",
    "#         minSize=(60, 60),\n",
    "#         flags=cv2.CASCADE_SCALE_IMAGE\n",
    "#     )\n",
    "\n",
    "#     for (x, y, w, h) in faces:\n",
    "#         cv2.rectangle(image,(x,y),(x+w,y+h),colors[0],2)\n",
    "    \n",
    "#     return image \n",
    "\n",
    "\n",
    "# def Main():\n",
    "#     Videocap = cv2.VideoCapture(0)\n",
    "\n",
    "#     while(True):\n",
    "#         ret, frame = Videocap.read()\n",
    "\n",
    "#         frame = face(frame)\n",
    "\n",
    "#         cv2.imshow('frame',frame)\n",
    "#         #press 'q' to end web cam.  \n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "#     Videocap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import the necessary packages\n",
    "# from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "# from tensorflow.keras.preprocessing.image import img_to_array\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from imutils.video import VideoStream\n",
    "# import numpy as np\n",
    "# import imutils\n",
    "# import time\n",
    "# from cv2 import cv2\n",
    "# import os\n",
    "\n",
    "# def detect_and_predict_mask(frame, faces, maskNet):\n",
    "# \t# grab the dimensions of the frame and then construct a blob\n",
    "# \t# from it\n",
    "# \t(h, w) = frame.shape[:2]\n",
    "# # \tblob = cv2.dnn.blobFromImage(frame, 1.0, (224, 224),\n",
    "# # \t\t(104.0, 177.0, 123.0))\n",
    "\n",
    "# \t# pass the blob through the network and obtain the face detections\n",
    "# # \tfaceNet.setInput(blob)\n",
    "# # \tdetections = faceNet.forward()\n",
    "# # \tprint(detections.shape)\n",
    "\n",
    "# \t# initialize our list of faces, their corresponding locations,\n",
    "# \t# and the list of predictions from our face mask network\n",
    "# \tfaces = []\n",
    "# \tlocs = []\n",
    "# \tpreds = []\n",
    "\n",
    "# \t# loop over the detections\n",
    "# # \tfor i in range(0, detections.shape[2]):\n",
    "# # \t\t# extract the confidence (i.e., probability) associated with\n",
    "# # \t\t# the detection\n",
    "# # \t\tconfidence = detections[0, 0, i, 2]\n",
    "\n",
    "# # \t\t# filter out weak detections by ensuring the confidence is\n",
    "# # \t\t# greater than the minimum confidence\n",
    "# # \t\tif confidence > 0.5:\n",
    "# # \t\t\t# compute the (x, y)-coordinates of the bounding box for\n",
    "# # \t\t\t# the object\n",
    "# # \t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "# # \t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "# # \t\t\t# ensure the bounding boxes fall within the dimensions of\n",
    "# # \t\t\t# the frame\n",
    "# # \t\t\t(startX, startY) = (max(0, startX), max(0, startY))\n",
    "# # \t\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
    "\n",
    "# # \t\t\t# extract the face ROI, convert it from BGR to RGB channel\n",
    "# # \t\t\t# ordering, resize it to 224x224, and preprocess it\n",
    "# # \t\t\tface = frame[startY:endY, startX:endX]\n",
    "# # \t\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "# # \t\t\tface = cv2.resize(face, (224, 224))\n",
    "# # \t\t\tface = img_to_array(face)\n",
    "# # \t\t\tface = preprocess_input(face)\n",
    "\n",
    "# # \t\t\t# add the face and bounding boxes to their respective\n",
    "# # \t\t\t# lists\n",
    "# # \t\t\tfaces.append(face)\n",
    "# # \t\t\tlocs.append((startX, startY, endX, endY))\n",
    "\n",
    "# \t# only make a predictions if at least one face was detected\n",
    "# \tif len(faces) > 0:\n",
    "# \t\t# for faster inference we'll make batch predictions on *all*\n",
    "# \t\t# faces at the same time rather than one-by-one predictions\n",
    "# \t\t# in the above `for` loop\n",
    "# \t\tfaces = np.array(faces, dtype=\"float32\")\n",
    "# \t\tpreds = maskNet.predict(faces, batch_size=32)\n",
    "\n",
    "# \t# return a 2-tuple of the face locations and their corresponding\n",
    "# \t# locations\n",
    "# \treturn (locs, preds)\n",
    "\n",
    "# # load our serialized face detector model from disk\n",
    "# prototxtPath = r\"face_detector\\deploy.prototxt\"\n",
    "# weightsPath = r\"face_detector\\res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "# faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
    "\n",
    "# # load the face mask detector model from disk\n",
    "# maskNet = load_model(\"mask_detector.model\")\n",
    "\n",
    "\n",
    "# # loop over the frames from the video stream\n",
    "# while True:\n",
    "# \t# grab the frame from the threaded video stream and resize it\n",
    "# \t# to have a maximum width of 400 pixels\n",
    "# \tframe = vs.read()\n",
    "# \tframe = imutils.resize(frame, width=400)\n",
    "\n",
    "# \t# detect faces in the frame and determine if they are wearing a\n",
    "# \t# face mask or not\n",
    "# \t(locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)\n",
    "\n",
    "# \t# loop over the detected face locations and their corresponding\n",
    "# \t# locations\n",
    "# \tfor (box, pred) in zip(locs, preds):\n",
    "# \t\t# unpack the bounding box and predictions\n",
    "# \t\t(startX, startY, endX, endY) = box\n",
    "# \t\t(mask, withoutMask) = pred\n",
    "\n",
    "# \t\t# determine the class label and color we'll use to draw\n",
    "# \t\t# the bounding box and text\n",
    "# \t\tlabel = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
    "# \t\tcolor = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
    "\n",
    "# \t\t# include the probability in the label\n",
    "# \t\tlabel = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
    "\n",
    "# \t\t# display the label and bounding box rectangle on the output\n",
    "# \t\t# frame\n",
    "# \t\tcv2.putText(frame, label, (startX, startY - 10),\n",
    "# \t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "# \t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
